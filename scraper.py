import requests
import base64
import os
import urllib3
from datetime import datetime

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

SOURCES = [
    "https://livpn.atwebpages.com/sub.php?token=3b4cbb400a537740",
    "https://subrostunnel.vercel.app/gen.txt",
    "https://gitverse.ru/api/repos/Vsevj/OBS/raw/branch/master/wwh",
    "https://raw.githubusercontent.com/CidVpn/cid-vpn-config/refs/heads/main/general.txt",
    "https://raw.githubusercontent.com/LimeHi/LimeVPN/refs/heads/main/LimeVPN.txt",
    "http://livpnsub.dpdns.org/sub.php?token=d712619499224ddb"
]

# Ğ¢Ğ¾Ñ‚ ÑĞ°Ğ¼Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ñ€Ğ°Ğ½ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°
COUNTRIES = {
    "GERMANY": "ğŸ‡©ğŸ‡ª DE", " DE ": "ğŸ‡©ğŸ‡ª DE",
    "USA": "ğŸ‡ºğŸ‡¸ US", " US ": "ğŸ‡ºğŸ‡¸ US", "UNITED STATES": "ğŸ‡ºğŸ‡¸ US",
    "RUSSIA": "ğŸ‡·ğŸ‡º RU", " RU ": "ğŸ‡·ğŸ‡º RU",
    "TURKEY": "ğŸ‡¹ğŸ‡· TR", " TR ": "ğŸ‡¹ğŸ‡· TR",
    "FRANCE": "ğŸ‡«ğŸ‡· FR", " FR ": "ğŸ‡«ğŸ‡· FR",
    "NETHERLANDS": "ğŸ‡³ğŸ‡± NL", " NL ": "ğŸ‡³ğŸ‡± NL",
    "FINLAND": "ğŸ‡«ğŸ‡® FI", " FI ": "ğŸ‡«ğŸ‡® FI",
    "GREAT BRITAIN": "ğŸ‡¬ğŸ‡§ GB", " UK ": "ğŸ‡¬ğŸ‡§ GB",
    "JAPAN": "ğŸ‡¯ğŸ‡µ JP", " JP ": "ğŸ‡¯ğŸ‡µ JP",
    "SINGAPORE": "ğŸ‡¸ğŸ‡¬ SG", " SG ": "ğŸ‡¸ğŸ‡¬ SG",
    "POLAND": "ğŸ‡µğŸ‡± PL", " PL ": "ğŸ‡µğŸ‡± PL",
    "CANADA": "ğŸ‡¨ğŸ‡¦ CA", " UA ": "ğŸ‡ºğŸ‡¦ UA", "UKRAINE": "ğŸ‡ºğŸ‡¦ UA"
}

def process_line(line, idx):
    line_upper = line.upper()
    proto = line.split("://")[0].upper()
    found_geo = "ğŸ³ï¸ UNKNOWN"
    
    for key, val in COUNTRIES.items():
        if key in line_upper:
            found_geo = val
            break
            
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ€Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ¸ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñƒ (Ğ²ÑĞµ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ #)
    base_part = line.split("#")[0]
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ñ‡Ğ¸ÑÑ‚Ğ¾Ğµ Ğ¸Ğ¼Ñ
    new_name = f"{found_geo} {proto} {idx}"
    return f"{base_part}#{new_name}", found_geo != "ğŸ³ï¸ UNKNOWN"

def scrape():
    unique_configs = set()
    print("ğŸš€ Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    
    for url in SOURCES:
        try:
            r = requests.get(url, timeout=10, verify=False)
            if r.status_code == 200:
                content = r.text
                try: content = base64.b64decode(content).decode('utf-8')
                except: pass
                
                for l in content.splitlines():
                    l = l.strip()
                    if any(l.startswith(p) for p in ['vless://', 'vmess://', 'trojan://', 'ss://']):
                        unique_configs.add(l)
        except: continue

    with_country = []
    without_country = []
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³
    for i, line in enumerate(list(unique_configs)):
        new_line, has_geo = process_line(line, i + 1)
        if has_geo:
            with_country.append(new_line)
        else:
            without_country.append(new_line)

    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ñ‹ Ğ¿Ğ¾ Ğ°Ğ»Ñ„Ğ°Ğ²Ğ¸Ñ‚Ñƒ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ
    final_list = sorted(with_country) + sorted(without_country)

    if final_list:
        with open("sub.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(final_list))
        print(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾: {len(final_list)}")
    else:
        print("âŒ ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾")

if __name__ == "__main__":
    scrape()
